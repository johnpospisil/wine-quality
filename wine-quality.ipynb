{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc30d0a3",
   "metadata": {},
   "source": [
    "# Wine Quality Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc9bf2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1599, 12), (4898, 12))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries and datasets\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "data_dir = Path('data')\n",
    "red = pd.read_csv(data_dir / 'winequality-red.csv', sep=';')\n",
    "white = pd.read_csv(data_dir / 'winequality-white.csv', sep=';')\n",
    "# Keep variables in global namespace for later cells\n",
    "red.shape, white.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c443836",
   "metadata": {},
   "source": [
    "## Initial Data Analysis\n",
    "\n",
    "Let's explore the structure and characteristics of both wine datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e608da56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "RED WINE DATASET\n",
      "============================================================\n",
      "Shape: 1599 rows × 12 columns\n",
      "\n",
      "Columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "\n",
      "============================================================\n",
      "WHITE WINE DATASET\n",
      "============================================================\n",
      "Shape: 4898 rows × 12 columns\n",
      "\n",
      "Columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n"
     ]
    }
   ],
   "source": [
    "# Dataset shapes and basic info\n",
    "print(\"=\" * 60)\n",
    "print(\"RED WINE DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {red.shape[0]} rows × {red.shape[1]} columns\\n\")\n",
    "print(\"Columns:\", list(red.columns))\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"WHITE WINE DATASET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {white.shape[0]} rows × {white.shape[1]} columns\\n\")\n",
    "print(\"Columns:\", list(white.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b193e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE - First 5 rows:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "WHITE WINE - First 5 rows:\n",
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "3            7.2              0.23         0.32             8.5      0.058   \n",
      "4            7.2              0.23         0.32             8.5      0.058   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n",
      "3      9.9        6  \n",
      "4      9.9        6  \n"
     ]
    }
   ],
   "source": [
    "# First few rows of each dataset\n",
    "print(\"RED WINE - First 5 rows:\")\n",
    "print(red.head())\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "print(\"WHITE WINE - First 5 rows:\")\n",
    "print(white.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627eae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE - Data Types and Missing Values:\n",
      "------------------------------------------------------------\n",
      "              Column Data Type  Non-Null Count  Missing\n",
      "       fixed acidity   float64            1599        0\n",
      "    volatile acidity   float64            1599        0\n",
      "         citric acid   float64            1599        0\n",
      "      residual sugar   float64            1599        0\n",
      "           chlorides   float64            1599        0\n",
      " free sulfur dioxide   float64            1599        0\n",
      "total sulfur dioxide   float64            1599        0\n",
      "             density   float64            1599        0\n",
      "                  pH   float64            1599        0\n",
      "           sulphates   float64            1599        0\n",
      "             alcohol   float64            1599        0\n",
      "             quality     int64            1599        0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "WHITE WINE - Data Types and Missing Values:\n",
      "------------------------------------------------------------\n",
      "              Column Data Type  Non-Null Count  Missing\n",
      "       fixed acidity   float64            4898        0\n",
      "    volatile acidity   float64            4898        0\n",
      "         citric acid   float64            4898        0\n",
      "      residual sugar   float64            4898        0\n",
      "           chlorides   float64            4898        0\n",
      " free sulfur dioxide   float64            4898        0\n",
      "total sulfur dioxide   float64            4898        0\n",
      "             density   float64            4898        0\n",
      "                  pH   float64            4898        0\n",
      "           sulphates   float64            4898        0\n",
      "             alcohol   float64            4898        0\n",
      "             quality     int64            4898        0\n"
     ]
    }
   ],
   "source": [
    "# Data types and missing values\n",
    "print(\"RED WINE - Data Types and Missing Values:\")\n",
    "print(\"-\" * 60)\n",
    "red_info = pd.DataFrame({\n",
    "    'Column': red.columns,\n",
    "    'Data Type': red.dtypes.values,\n",
    "    'Non-Null Count': red.count().values,\n",
    "    'Missing': red.isnull().sum().values\n",
    "})\n",
    "print(red_info.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"WHITE WINE - Data Types and Missing Values:\")\n",
    "print(\"-\" * 60)\n",
    "white_info = pd.DataFrame({\n",
    "    'Column': white.columns,\n",
    "    'Data Type': white.dtypes.values,\n",
    "    'Non-Null Count': white.count().values,\n",
    "    'Missing': white.isnull().sum().values\n",
    "})\n",
    "print(white_info.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069026f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RED WINE - Quality Distribution:\n",
      "------------------------------------------------------------\n",
      "quality\n",
      "3     10\n",
      "4     53\n",
      "5    681\n",
      "6    638\n",
      "7    199\n",
      "8     18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mean Quality: 5.64\n",
      "Median Quality: 6.0\n",
      "Quality Range: 3 - 8\n",
      "\n",
      "================================================================================\n",
      "\n",
      "WHITE WINE - Quality Distribution:\n",
      "------------------------------------------------------------\n",
      "quality\n",
      "3      20\n",
      "4     163\n",
      "5    1457\n",
      "6    2198\n",
      "7     880\n",
      "8     175\n",
      "9       5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Mean Quality: 5.88\n",
      "Median Quality: 6.0\n",
      "Quality Range: 3 - 9\n"
     ]
    }
   ],
   "source": [
    "# Quality distribution (target variable)\n",
    "print(\"RED WINE - Quality Distribution:\")\n",
    "print(\"-\" * 60)\n",
    "red_quality = red['quality'].value_counts().sort_index()\n",
    "print(red_quality)\n",
    "print(f\"\\nMean Quality: {red['quality'].mean():.2f}\")\n",
    "print(f\"Median Quality: {red['quality'].median():.1f}\")\n",
    "print(f\"Quality Range: {red['quality'].min()} - {red['quality'].max()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"WHITE WINE - Quality Distribution:\")\n",
    "print(\"-\" * 60)\n",
    "white_quality = white['quality'].value_counts().sort_index()\n",
    "print(white_quality)\n",
    "print(f\"\\nMean Quality: {white['quality'].mean():.2f}\")\n",
    "print(f\"Median Quality: {white['quality'].median():.1f}\")\n",
    "print(f\"Quality Range: {white['quality'].min()} - {white['quality'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5259c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUPLICATE ROWS CHECK:\n",
      "------------------------------------------------------------\n",
      "Red wine duplicates: 240\n",
      "White wine duplicates: 937\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"DUPLICATE ROWS CHECK:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Red wine duplicates: {red.duplicated().sum()}\")\n",
    "print(f\"White wine duplicates: {white.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765f2188",
   "metadata": {},
   "source": [
    "## Phase 1: Data Preparation & Preprocessing\n",
    "\n",
    "Now we'll prepare the data for modeling by:\n",
    "1. Combining datasets with wine type indicator\n",
    "2. Handling duplicates\n",
    "3. Creating train/test splits\n",
    "4. Scaling features\n",
    "5. Creating different target variable formats (regression, multi-class, binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208250d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries needed for preprocessing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915d9a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Creating Combined Dataset\n",
      "======================================================================\n",
      "Combined dataset shape: (6497, 13)\n",
      "  Red wines:   1,599 samples\n",
      "  White wines: 4,898 samples\n",
      "  Total:       6,497 samples\n",
      "\n",
      "Features: 11 (excluding quality and wine_type)\n",
      "Columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality', 'wine_type']\n",
      "\n",
      "Wine type encoding: Red=0, White=1\n",
      "wine_type  wine_type_encoded\n",
      "white      1                    4898\n",
      "red        0                    1599\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create combined dataset with wine_type indicator\n",
    "print(\"STEP 1: Creating Combined Dataset\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Add wine_type column\n",
    "red_with_type = red.copy()\n",
    "red_with_type['wine_type'] = 'red'\n",
    "\n",
    "white_with_type = white.copy()\n",
    "white_with_type['wine_type'] = 'white'\n",
    "\n",
    "# Combine datasets\n",
    "wine_combined = pd.concat([red_with_type, white_with_type], axis=0, ignore_index=True)\n",
    "\n",
    "print(f\"Combined dataset shape: {wine_combined.shape}\")\n",
    "print(f\"  Red wines:   {len(red_with_type):,} samples\")\n",
    "print(f\"  White wines: {len(white_with_type):,} samples\")\n",
    "print(f\"  Total:       {len(wine_combined):,} samples\")\n",
    "print(f\"\\nFeatures: {wine_combined.shape[1] - 2} (excluding quality and wine_type)\")\n",
    "print(f\"Columns: {list(wine_combined.columns)}\")\n",
    "\n",
    "# Convert wine_type to numeric (0=red, 1=white)\n",
    "wine_combined['wine_type_encoded'] = (wine_combined['wine_type'] == 'white').astype(int)\n",
    "\n",
    "print(f\"\\nWine type encoding: Red=0, White=1\")\n",
    "print(wine_combined[['wine_type', 'wine_type_encoded']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db11c8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 2: Handling Duplicate Rows\n",
      "======================================================================\n",
      "Duplicate rows found: 1177\n",
      "  Red wine duplicates:   240\n",
      "  White wine duplicates: 937\n",
      "\n",
      "After removing duplicates: 5,320 samples\n",
      "Removed: 1177 rows (22.12%)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Handle duplicates\n",
    "print(\"\\nSTEP 2: Handling Duplicate Rows\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "duplicates_before = wine_combined.duplicated().sum()\n",
    "print(f\"Duplicate rows found: {duplicates_before}\")\n",
    "\n",
    "if duplicates_before > 0:\n",
    "    # Check duplicates by wine type\n",
    "    red_dupes = wine_combined[wine_combined['wine_type'] == 'red'].duplicated().sum()\n",
    "    white_dupes = wine_combined[wine_combined['wine_type'] == 'white'].duplicated().sum()\n",
    "    print(f\"  Red wine duplicates:   {red_dupes}\")\n",
    "    print(f\"  White wine duplicates: {white_dupes}\")\n",
    "    \n",
    "    # Remove duplicates\n",
    "    wine_combined = wine_combined.drop_duplicates()\n",
    "    print(f\"\\nAfter removing duplicates: {wine_combined.shape[0]:,} samples\")\n",
    "    print(f\"Removed: {duplicates_before} rows ({duplicates_before/len(wine_combined)*100:.2f}%)\")\n",
    "else:\n",
    "    print(\"No duplicates found - data is clean!\")\n",
    "\n",
    "# Reset index after dropping duplicates\n",
    "wine_combined = wine_combined.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe10525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 3: Creating Target Variable Formats\n",
      "======================================================================\n",
      "Target variable formats created:\n",
      "\n",
      "1. REGRESSION (quality_original):\n",
      "   Range: 3 to 9\n",
      "   Mean: 5.796\n",
      "   Std: 0.880\n",
      "\n",
      "2. BINARY CLASSIFICATION (quality_binary):\n",
      "   Not Good (0, quality <7):  4,311 samples (81.0%)\n",
      "   Good (1, quality >=7):     1,009 samples (19.0%)\n",
      "\n",
      "3. MULTI-CLASS CLASSIFICATION (quality_multiclass):\n",
      "   Classes: [3, 4, 5, 6, 7, 8, 9]\n",
      "   Distribution:\n",
      "     Quality 3:    30 (  0.6%)\n",
      "     Quality 4:   206 (  3.9%)\n",
      "     Quality 5: 1,752 ( 32.9%)\n",
      "     Quality 6: 2,323 ( 43.7%)\n",
      "     Quality 7:   856 ( 16.1%)\n",
      "     Quality 8:   148 (  2.8%)\n",
      "     Quality 9:     5 (  0.1%)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create different target variable formats\n",
    "print(\"\\nSTEP 3: Creating Target Variable Formats\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Original quality (for regression)\n",
    "wine_combined['quality_original'] = wine_combined['quality']\n",
    "\n",
    "# Binary classification: quality >= 7 is \"good\" (1), otherwise \"not good\" (0)\n",
    "wine_combined['quality_binary'] = (wine_combined['quality'] >= 7).astype(int)\n",
    "\n",
    "# Multi-class (keep original quality scores 3-9)\n",
    "wine_combined['quality_multiclass'] = wine_combined['quality']\n",
    "\n",
    "print(\"Target variable formats created:\")\n",
    "print(\"\\n1. REGRESSION (quality_original):\")\n",
    "print(f\"   Range: {wine_combined['quality_original'].min()} to {wine_combined['quality_original'].max()}\")\n",
    "print(f\"   Mean: {wine_combined['quality_original'].mean():.3f}\")\n",
    "print(f\"   Std: {wine_combined['quality_original'].std():.3f}\")\n",
    "\n",
    "print(\"\\n2. BINARY CLASSIFICATION (quality_binary):\")\n",
    "print(f\"   Not Good (0, quality <7):  {(wine_combined['quality_binary'] == 0).sum():,} samples ({(wine_combined['quality_binary'] == 0).sum()/len(wine_combined)*100:.1f}%)\")\n",
    "print(f\"   Good (1, quality >=7):     {(wine_combined['quality_binary'] == 1).sum():,} samples ({(wine_combined['quality_binary'] == 1).sum()/len(wine_combined)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n3. MULTI-CLASS CLASSIFICATION (quality_multiclass):\")\n",
    "print(f\"   Classes: {sorted(wine_combined['quality_multiclass'].unique())}\")\n",
    "print(f\"   Distribution:\")\n",
    "for quality, count in wine_combined['quality_multiclass'].value_counts().sort_index().items():\n",
    "    pct = count / len(wine_combined) * 100\n",
    "    print(f\"     Quality {quality}: {count:5,} ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b62a599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 4: Defining Feature Columns\n",
      "======================================================================\n",
      "Original features (11): ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n",
      "\n",
      "With wine type (12): ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'wine_type_encoded']\n",
      "\n",
      "Feature ranges:\n",
      "     fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "min            3.8              0.08         0.00             0.6      0.009   \n",
      "max           15.9              1.58         1.66            65.8      0.611   \n",
      "\n",
      "     free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "min                  1.0                   6.0  0.98711  2.72       0.22   \n",
      "max                289.0                 440.0  1.03898  4.01       2.00   \n",
      "\n",
      "     alcohol  \n",
      "min      8.0  \n",
      "max     14.9  \n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define feature columns (exclude target and metadata)\n",
    "print(\"\\nSTEP 4: Defining Feature Columns\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Original features (chemical properties)\n",
    "feature_cols_original = [\n",
    "    'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
    "    'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
    "    'pH', 'sulphates', 'alcohol'\n",
    "]\n",
    "\n",
    "# Features with wine type\n",
    "feature_cols_with_type = feature_cols_original + ['wine_type_encoded']\n",
    "\n",
    "print(f\"Original features (11): {feature_cols_original}\")\n",
    "print(f\"\\nWith wine type (12): {feature_cols_with_type}\")\n",
    "print(f\"\\nFeature ranges:\")\n",
    "print(wine_combined[feature_cols_original].describe().loc[['min', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6002907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 5: Creating Train/Test Splits (80/20)\n",
      "======================================================================\n",
      "Training set:   4,256 samples (80.0%)\n",
      "Test set:       1,064 samples (20.0%)\n",
      "\n",
      "Feature shape: (4256, 12)\n",
      "\n",
      "Quality distribution preserved in splits:\n",
      "\n",
      "Training set:\n",
      "quality_multiclass\n",
      "3      24\n",
      "4     165\n",
      "5    1402\n",
      "6    1858\n",
      "7     685\n",
      "8     118\n",
      "9       4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set:\n",
      "quality_multiclass\n",
      "3      6\n",
      "4     41\n",
      "5    350\n",
      "6    465\n",
      "7    171\n",
      "8     30\n",
      "9      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create train/test splits (stratified by quality)\n",
    "print(\"\\nSTEP 5: Creating Train/Test Splits (80/20)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Split with stratification on quality to maintain distribution\n",
    "X = wine_combined[feature_cols_with_type]\n",
    "y_regression = wine_combined['quality_original']\n",
    "y_binary = wine_combined['quality_binary']\n",
    "y_multiclass = wine_combined['quality_multiclass']\n",
    "\n",
    "# Use multiclass for stratification (most granular)\n",
    "X_train, X_test, y_reg_train, y_reg_test, y_bin_train, y_bin_test, y_multi_train, y_multi_test = train_test_split(\n",
    "    X, y_regression, y_binary, y_multiclass,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_multiclass\n",
    ")\n",
    "\n",
    "print(f\"Training set:   {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFeature shape: {X_train.shape}\")\n",
    "print(f\"\\nQuality distribution preserved in splits:\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(y_multi_train.value_counts().sort_index())\n",
    "print(\"\\nTest set:\")\n",
    "print(y_multi_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c0981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 6: Feature Scaling (Standardization)\n",
      "======================================================================\n",
      "Features scaled using StandardScaler (mean=0, std=1)\n",
      "\n",
      "Before scaling (training set):\n",
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "mean          7.225             0.343        0.319           5.002      0.057   \n",
      "std           1.332             0.167        0.147           4.450      0.036   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density     pH  sulphates  \\\n",
      "mean               29.992               113.737    0.995  3.224      0.533   \n",
      "std                17.824                56.554    0.003  0.160      0.146   \n",
      "\n",
      "      alcohol  wine_type_encoded  \n",
      "mean   10.568              0.743  \n",
      "std     1.191              0.437  \n",
      "\n",
      "After scaling (training set):\n",
      "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "mean           -0.0               0.0         -0.0            -0.0       -0.0   \n",
      "std             1.0               1.0          1.0             1.0        1.0   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density   pH  sulphates  \\\n",
      "mean                  0.0                  -0.0     -0.0  0.0        0.0   \n",
      "std                   1.0                   1.0      1.0  1.0        1.0   \n",
      "\n",
      "      alcohol  wine_type_encoded  \n",
      "mean      0.0               -0.0  \n",
      "std       1.0                1.0  \n",
      "\n",
      "✓ Scaling complete - data is ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Feature scaling (standardization)\n",
    "print(\"\\nSTEP 6: Feature Scaling (Standardization)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only (prevent data leakage)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier use\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Features scaled using StandardScaler (mean=0, std=1)\")\n",
    "print(\"\\nBefore scaling (training set):\")\n",
    "print(X_train.describe().loc[['mean', 'std']].round(3))\n",
    "print(\"\\nAfter scaling (training set):\")\n",
    "print(X_train_scaled.describe().loc[['mean', 'std']].round(3))\n",
    "\n",
    "print(\"\\n✓ Scaling complete - data is ready for modeling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd777010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STEP 7: Creating Wine-Specific Datasets\n",
      "======================================================================\n",
      "Red wine datasets created:\n",
      "  Train: 1,092 samples × 11 features\n",
      "  Test:  267 samples × 11 features\n",
      "\n",
      "White wine datasets created:\n",
      "  Train: 3,164 samples × 11 features\n",
      "  Test:  797 samples × 11 features\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Create separate datasets for wine-specific models\n",
    "print(\"\\nSTEP 7: Creating Wine-Specific Datasets\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Red wine only datasets\n",
    "red_indices_train = X_train[X_train['wine_type_encoded'] == 0].index\n",
    "red_indices_test = X_test[X_test['wine_type_encoded'] == 0].index\n",
    "\n",
    "X_train_red = X_train.loc[red_indices_train, feature_cols_original]\n",
    "X_test_red = X_test.loc[red_indices_test, feature_cols_original]\n",
    "X_train_red_scaled = X_train_scaled.loc[red_indices_train, feature_cols_original]\n",
    "X_test_red_scaled = X_test_scaled.loc[red_indices_test, feature_cols_original]\n",
    "\n",
    "y_reg_train_red = y_reg_train.loc[red_indices_train]\n",
    "y_reg_test_red = y_reg_test.loc[red_indices_test]\n",
    "y_bin_train_red = y_bin_train.loc[red_indices_train]\n",
    "y_bin_test_red = y_bin_test.loc[red_indices_test]\n",
    "y_multi_train_red = y_multi_train.loc[red_indices_train]\n",
    "y_multi_test_red = y_multi_test.loc[red_indices_test]\n",
    "\n",
    "# White wine only datasets\n",
    "white_indices_train = X_train[X_train['wine_type_encoded'] == 1].index\n",
    "white_indices_test = X_test[X_test['wine_type_encoded'] == 1].index\n",
    "\n",
    "X_train_white = X_train.loc[white_indices_train, feature_cols_original]\n",
    "X_test_white = X_test.loc[white_indices_test, feature_cols_original]\n",
    "X_train_white_scaled = X_train_scaled.loc[white_indices_train, feature_cols_original]\n",
    "X_test_white_scaled = X_test_scaled.loc[white_indices_test, feature_cols_original]\n",
    "\n",
    "y_reg_train_white = y_reg_train.loc[white_indices_train]\n",
    "y_reg_test_white = y_reg_test.loc[white_indices_test]\n",
    "y_bin_train_white = y_bin_train.loc[white_indices_train]\n",
    "y_bin_test_white = y_bin_test.loc[white_indices_test]\n",
    "y_multi_train_white = y_multi_train.loc[white_indices_train]\n",
    "y_multi_test_white = y_multi_test.loc[white_indices_test]\n",
    "\n",
    "print(\"Red wine datasets created:\")\n",
    "print(f\"  Train: {X_train_red.shape[0]:,} samples × {X_train_red.shape[1]} features\")\n",
    "print(f\"  Test:  {X_test_red.shape[0]:,} samples × {X_test_red.shape[1]} features\")\n",
    "\n",
    "print(\"\\nWhite wine datasets created:\")\n",
    "print(f\"  Train: {X_train_white.shape[0]:,} samples × {X_train_white.shape[1]} features\")\n",
    "print(f\"  Test:  {X_test_white.shape[0]:,} samples × {X_test_white.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "381f8b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PHASE 1 COMPLETE: DATA PREPARATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "📊 DATASETS AVAILABLE FOR MODELING:\n",
      "\n",
      "1. COMBINED DATASET (Red + White):\n",
      "   • Features: 12 (including wine_type_encoded)\n",
      "   • Train: 4,256 samples\n",
      "   • Test:  1,064 samples\n",
      "\n",
      "2. RED WINE ONLY:\n",
      "   • Features: 11\n",
      "   • Train: 1,092 samples\n",
      "   • Test:  267 samples\n",
      "\n",
      "3. WHITE WINE ONLY:\n",
      "   • Features: 11\n",
      "   • Train: 3,164 samples\n",
      "   • Test:  797 samples\n",
      "\n",
      "🎯 TARGET VARIABLES:\n",
      "   • y_reg (regression): continuous quality scores\n",
      "   • y_bin (binary): good (≥7) vs not good (<7)\n",
      "   • y_multi (multi-class): quality classes 3-9\n",
      "\n",
      "🔧 DATA VARIATIONS:\n",
      "   • X_train, X_test: Unscaled features\n",
      "   • X_train_scaled, X_test_scaled: Standardized features (mean=0, std=1)\n",
      "\n",
      "✅ READY FOR PHASE 2: Baseline Regression Models\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary: All prepared datasets\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PHASE 1 COMPLETE: DATA PREPARATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n📊 DATASETS AVAILABLE FOR MODELING:\\n\")\n",
    "\n",
    "print(\"1. COMBINED DATASET (Red + White):\")\n",
    "print(f\"   • Features: {X_train.shape[1]} (including wine_type_encoded)\")\n",
    "print(f\"   • Train: {X_train.shape[0]:,} samples\")\n",
    "print(f\"   • Test:  {X_test.shape[0]:,} samples\")\n",
    "\n",
    "print(\"\\n2. RED WINE ONLY:\")\n",
    "print(f\"   • Features: {X_train_red.shape[1]}\")\n",
    "print(f\"   • Train: {X_train_red.shape[0]:,} samples\")\n",
    "print(f\"   • Test:  {X_test_red.shape[0]:,} samples\")\n",
    "\n",
    "print(\"\\n3. WHITE WINE ONLY:\")\n",
    "print(f\"   • Features: {X_train_white.shape[1]}\")\n",
    "print(f\"   • Train: {X_train_white.shape[0]:,} samples\")\n",
    "print(f\"   • Test:  {X_test_white.shape[0]:,} samples\")\n",
    "\n",
    "print(\"\\n🎯 TARGET VARIABLES:\")\n",
    "print(\"   • y_reg (regression): continuous quality scores\")\n",
    "print(\"   • y_bin (binary): good (≥7) vs not good (<7)\")\n",
    "print(\"   • y_multi (multi-class): quality classes 3-9\")\n",
    "\n",
    "print(\"\\n🔧 DATA VARIATIONS:\")\n",
    "print(\"   • X_train, X_test: Unscaled features\")\n",
    "print(\"   • X_train_scaled, X_test_scaled: Standardized features (mean=0, std=1)\")\n",
    "\n",
    "print(\"\\n✅ READY FOR PHASE 2: Baseline Regression Models\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65652b99",
   "metadata": {},
   "source": [
    "## Phase 2: Baseline Regression Models\n",
    "\n",
    "We'll establish performance benchmarks using three linear regression approaches:\n",
    "1. **Linear Regression**: Simple baseline\n",
    "2. **Ridge Regression**: L2 regularization (handles multicollinearity)\n",
    "3. **Lasso Regression**: L1 regularization (feature selection)\n",
    "\n",
    "Each model will be trained on three dataset variations:\n",
    "- Combined (red + white with wine_type)\n",
    "- Red wine only\n",
    "- White wine only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea4c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression libraries imported successfully!\n",
      "Models: LinearRegression, Ridge, Lasso\n",
      "Metrics: MAE, RMSE, R²\n"
     ]
    }
   ],
   "source": [
    "# Import regression models and evaluation metrics\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import time\n",
    "\n",
    "print(\"Regression libraries imported successfully!\")\n",
    "print(\"Models: LinearRegression, Ridge, Lasso\")\n",
    "print(\"Metrics: MAE, RMSE, R²\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0b59df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined!\n",
      "Metrics tracked: MAE, RMSE, R², Training Time\n"
     ]
    }
   ],
   "source": [
    "# Helper function to evaluate regression models\n",
    "def evaluate_regression_model(model, X_train, X_test, y_train, y_test, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a regression model, return metrics\n",
    "    \"\"\"\n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'Train_MAE': mean_absolute_error(y_train, y_train_pred),\n",
    "        'Test_MAE': mean_absolute_error(y_test, y_test_pred),\n",
    "        'Train_RMSE': np.sqrt(mean_squared_error(y_train, y_train_pred)),\n",
    "        'Test_RMSE': np.sqrt(mean_squared_error(y_test, y_test_pred)),\n",
    "        'Train_R2': r2_score(y_train, y_train_pred),\n",
    "        'Test_R2': r2_score(y_test, y_test_pred),\n",
    "        'Train_Time_sec': train_time,\n",
    "        'Model_Object': model\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Evaluation function defined!\")\n",
    "print(\"Metrics tracked: MAE, RMSE, R², Training Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf270c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 1: LINEAR REGRESSION\n",
      "================================================================================\n",
      "\n",
      "1. Training on COMBINED dataset (Red + White)...\n",
      "   ✓ Test MAE: 0.5660 | Test R²: 0.3134\n",
      "\n",
      "2. Training on RED WINE dataset...\n",
      "   ✓ Test MAE: 0.4755 | Test R²: 0.3750\n",
      "\n",
      "3. Training on WHITE WINE dataset...\n",
      "   ✓ Test MAE: 0.5949 | Test R²: 0.2792\n",
      "\n",
      "✓ Linear Regression training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Linear Regression on all datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_lr = []\n",
    "\n",
    "# Combined dataset\n",
    "print(\"\\n1. Training on COMBINED dataset (Red + White)...\")\n",
    "lr_combined = LinearRegression()\n",
    "metrics = evaluate_regression_model(\n",
    "    lr_combined, X_train_scaled, X_test_scaled, \n",
    "    y_reg_train, y_reg_test,\n",
    "    'Linear Regression', 'Combined'\n",
    ")\n",
    "results_lr.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "# Red wine only\n",
    "print(\"\\n2. Training on RED WINE dataset...\")\n",
    "lr_red = LinearRegression()\n",
    "metrics = evaluate_regression_model(\n",
    "    lr_red, X_train_red_scaled, X_test_red_scaled,\n",
    "    y_reg_train_red, y_reg_test_red,\n",
    "    'Linear Regression', 'Red Only'\n",
    ")\n",
    "results_lr.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "# White wine only\n",
    "print(\"\\n3. Training on WHITE WINE dataset...\")\n",
    "lr_white = LinearRegression()\n",
    "metrics = evaluate_regression_model(\n",
    "    lr_white, X_train_white_scaled, X_test_white_scaled,\n",
    "    y_reg_train_white, y_reg_test_white,\n",
    "    'Linear Regression', 'White Only'\n",
    ")\n",
    "results_lr.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Linear Regression training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a154bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 2: RIDGE REGRESSION (L2 Regularization)\n",
      "================================================================================\n",
      "\n",
      "1. Training on COMBINED dataset (Red + White)...\n",
      "   ✓ Test MAE: 0.5660 | Test R²: 0.3134\n",
      "\n",
      "2. Training on RED WINE dataset...\n",
      "   ✓ Test MAE: 0.4755 | Test R²: 0.3754\n",
      "\n",
      "3. Training on WHITE WINE dataset...\n",
      "   ✓ Test MAE: 0.5949 | Test R²: 0.2792\n",
      "\n",
      "✓ Ridge Regression training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Ridge Regression (L2 regularization, alpha=1.0)\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 2: RIDGE REGRESSION (L2 Regularization)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_ridge = []\n",
    "\n",
    "# Combined dataset\n",
    "print(\"\\n1. Training on COMBINED dataset (Red + White)...\")\n",
    "ridge_combined = Ridge(alpha=1.0, random_state=42)\n",
    "metrics = evaluate_regression_model(\n",
    "    ridge_combined, X_train_scaled, X_test_scaled,\n",
    "    y_reg_train, y_reg_test,\n",
    "    'Ridge', 'Combined'\n",
    ")\n",
    "results_ridge.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "# Red wine only\n",
    "print(\"\\n2. Training on RED WINE dataset...\")\n",
    "ridge_red = Ridge(alpha=1.0, random_state=42)\n",
    "metrics = evaluate_regression_model(\n",
    "    ridge_red, X_train_red_scaled, X_test_red_scaled,\n",
    "    y_reg_train_red, y_reg_test_red,\n",
    "    'Ridge', 'Red Only'\n",
    ")\n",
    "results_ridge.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "# White wine only\n",
    "print(\"\\n3. Training on WHITE WINE dataset...\")\n",
    "ridge_white = Ridge(alpha=1.0, random_state=42)\n",
    "metrics = evaluate_regression_model(\n",
    "    ridge_white, X_train_white_scaled, X_test_white_scaled,\n",
    "    y_reg_train_white, y_reg_test_white,\n",
    "    'Ridge', 'White Only'\n",
    ")\n",
    "results_ridge.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Ridge Regression training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6583af15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 3: LASSO REGRESSION (L1 Regularization)\n",
      "================================================================================\n",
      "\n",
      "1. Training on COMBINED dataset (Red + White)...\n",
      "   ✓ Test MAE: 0.5688 | Test R²: 0.3046\n",
      "\n",
      "2. Training on RED WINE dataset...\n",
      "   ✓ Test MAE: 0.4746 | Test R²: 0.3910\n",
      "\n",
      "3. Training on WHITE WINE dataset...\n",
      "   ✓ Test MAE: 0.5966 | Test R²: 0.2750\n",
      "\n",
      "✓ Lasso Regression training complete!\n",
      "   ✓ Test MAE: 0.4746 | Test R²: 0.3910\n",
      "\n",
      "3. Training on WHITE WINE dataset...\n",
      "   ✓ Test MAE: 0.5966 | Test R²: 0.2750\n",
      "\n",
      "✓ Lasso Regression training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model 3: Lasso Regression (L1 regularization, alpha=0.01)\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 3: LASSO REGRESSION (L1 Regularization)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_lasso = []\n",
    "\n",
    "# Combined dataset\n",
    "print(\"\\n1. Training on COMBINED dataset (Red + White)...\")\n",
    "lasso_combined = Lasso(alpha=0.01, random_state=42, max_iter=10000)\n",
    "metrics = evaluate_regression_model(\n",
    "    lasso_combined, X_train_scaled, X_test_scaled,\n",
    "    y_reg_train, y_reg_test,\n",
    "    'Lasso', 'Combined'\n",
    ")\n",
    "results_lasso.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "# Red wine only\n",
    "print(\"\\n2. Training on RED WINE dataset...\")\n",
    "lasso_red = Lasso(alpha=0.01, random_state=42, max_iter=10000)\n",
    "metrics = evaluate_regression_model(\n",
    "    lasso_red, X_train_red_scaled, X_test_red_scaled,\n",
    "    y_reg_train_red, y_reg_test_red,\n",
    "    'Lasso', 'Red Only'\n",
    ")\n",
    "results_lasso.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "# White wine only\n",
    "print(\"\\n3. Training on WHITE WINE dataset...\")\n",
    "lasso_white = Lasso(alpha=0.01, random_state=42, max_iter=10000)\n",
    "metrics = evaluate_regression_model(\n",
    "    lasso_white, X_train_white_scaled, X_test_white_scaled,\n",
    "    y_reg_train_white, y_reg_test_white,\n",
    "    'Lasso', 'White Only'\n",
    ")\n",
    "results_lasso.append(metrics)\n",
    "print(f\"   ✓ Test MAE: {metrics['Test_MAE']:.4f} | Test R²: {metrics['Test_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Lasso Regression training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479c45fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BASELINE REGRESSION MODELS - COMPLETE RESULTS\n",
      "================================================================================\n",
      "\n",
      "Test Set Performance:\n",
      "            Model    Dataset  Test_MAE  Test_RMSE  Test_R2  Train_Time_sec\n",
      "Linear Regression   Combined    0.5660     0.7292   0.3134          0.0017\n",
      "Linear Regression   Red Only    0.4755     0.6156   0.3750          0.0013\n",
      "Linear Regression White Only    0.5949     0.7660   0.2792          0.0007\n",
      "            Ridge   Combined    0.5660     0.7293   0.3134          0.0015\n",
      "            Ridge   Red Only    0.4755     0.6155   0.3754          0.0005\n",
      "            Ridge White Only    0.5949     0.7660   0.2792          0.0004\n",
      "            Lasso   Combined    0.5688     0.7339   0.3046          0.0022\n",
      "            Lasso   Red Only    0.4746     0.6077   0.3910          0.0018\n",
      "            Lasso White Only    0.5966     0.7682   0.2750          0.0027\n",
      "\n",
      "================================================================================\n",
      "🏆 BEST BASELINE MODEL:\n",
      "================================================================================\n",
      "Model:    Lasso\n",
      "Dataset:  Red Only\n",
      "Test MAE: 0.4746\n",
      "Test RMSE: 0.6077\n",
      "Test R²:  0.3910\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Combine all results and create comparison table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BASELINE REGRESSION MODELS - COMPLETE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine all results\n",
    "all_results = results_lr + results_ridge + results_lasso\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Select key columns for display\n",
    "display_cols = ['Model', 'Dataset', 'Test_MAE', 'Test_RMSE', 'Test_R2', 'Train_Time_sec']\n",
    "results_display = results_df[display_cols].copy()\n",
    "\n",
    "# Format for better readability\n",
    "results_display['Test_MAE'] = results_display['Test_MAE'].round(4)\n",
    "results_display['Test_RMSE'] = results_display['Test_RMSE'].round(4)\n",
    "results_display['Test_R2'] = results_display['Test_R2'].round(4)\n",
    "results_display['Train_Time_sec'] = results_display['Train_Time_sec'].round(4)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(results_display.to_string(index=False))\n",
    "\n",
    "# Find best model by Test MAE\n",
    "best_idx = results_df['Test_MAE'].idxmin()\n",
    "best_model = results_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🏆 BEST BASELINE MODEL:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model:    {best_model['Model']}\")\n",
    "print(f\"Dataset:  {best_model['Dataset']}\")\n",
    "print(f\"Test MAE: {best_model['Test_MAE']:.4f}\")\n",
    "print(f\"Test RMSE: {best_model['Test_RMSE']:.4f}\")\n",
    "print(f\"Test R²:  {best_model['Test_R2']:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2588a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAIN VS TEST PERFORMANCE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "MAE Comparison (lower is better):\n",
      "            Model    Dataset  Train_MAE  Test_MAE  MAE_Gap\n",
      "Linear Regression   Combined   0.563108  0.566001   0.0029\n",
      "Linear Regression   Red Only   0.516118  0.475545  -0.0406\n",
      "Linear Regression White Only   0.572018  0.594922   0.0229\n",
      "            Ridge   Combined   0.563113  0.566005   0.0029\n",
      "            Ridge   Red Only   0.516093  0.475494  -0.0406\n",
      "            Ridge White Only   0.572033  0.594929   0.0229\n",
      "            Lasso   Combined   0.567006  0.568828   0.0018\n",
      "            Lasso   Red Only   0.518292  0.474589  -0.0437\n",
      "            Lasso White Only   0.575391  0.596575   0.0212\n",
      "\n",
      "\n",
      "R² Comparison (higher is better):\n",
      "            Model    Dataset  Train_R2  Test_R2  R2_Gap\n",
      "Linear Regression   Combined  0.309975 0.313440 -0.0035\n",
      "Linear Regression   Red Only  0.356967 0.375026 -0.0181\n",
      "Linear Regression White Only  0.304165 0.279240  0.0249\n",
      "            Ridge   Combined  0.309975 0.313419 -0.0034\n",
      "            Ridge   Red Only  0.356965 0.375364 -0.0184\n",
      "            Ridge White Only  0.304164 0.279197  0.0250\n",
      "            Lasso   Combined  0.302445 0.304553 -0.0021\n",
      "            Lasso   Red Only  0.353084 0.391012 -0.0379\n",
      "            Lasso White Only  0.299150 0.275014  0.0241\n",
      "\n",
      "📊 INTERPRETATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Average MAE gap (Test - Train): -0.0056\n",
      "Average R² gap (Train - Test): -0.0010\n",
      "✓ Models generalize well - low overfitting\n"
     ]
    }
   ],
   "source": [
    "# Analyze train vs test performance (check for overfitting/underfitting)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAIN VS TEST PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = results_df[['Model', 'Dataset', 'Train_MAE', 'Test_MAE', 'Train_R2', 'Test_R2']].copy()\n",
    "\n",
    "# Calculate gap between train and test (indicator of overfitting)\n",
    "comparison_df['MAE_Gap'] = (comparison_df['Test_MAE'] - comparison_df['Train_MAE']).round(4)\n",
    "comparison_df['R2_Gap'] = (comparison_df['Train_R2'] - comparison_df['Test_R2']).round(4)\n",
    "\n",
    "print(\"\\nMAE Comparison (lower is better):\")\n",
    "print(comparison_df[['Model', 'Dataset', 'Train_MAE', 'Test_MAE', 'MAE_Gap']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nR² Comparison (higher is better):\")\n",
    "print(comparison_df[['Model', 'Dataset', 'Train_R2', 'Test_R2', 'R2_Gap']].to_string(index=False))\n",
    "\n",
    "print(\"\\n📊 INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "avg_mae_gap = comparison_df['MAE_Gap'].mean()\n",
    "avg_r2_gap = comparison_df['R2_Gap'].mean()\n",
    "\n",
    "print(f\"Average MAE gap (Test - Train): {avg_mae_gap:.4f}\")\n",
    "print(f\"Average R² gap (Train - Test): {avg_r2_gap:.4f}\")\n",
    "\n",
    "if avg_mae_gap < 0.05 and avg_r2_gap < 0.05:\n",
    "    print(\"✓ Models generalize well - low overfitting\")\n",
    "elif avg_mae_gap > 0.15 or avg_r2_gap > 0.15:\n",
    "    print(\"⚠ Potential overfitting detected - consider regularization or simpler models\")\n",
    "else:\n",
    "    print(\"✓ Acceptable generalization - models perform reasonably on unseen data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b45712f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DATASET COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Average Performance by Dataset (across all 3 models):\n",
      "            Test_MAE  Test_RMSE  Test_R2\n",
      "Dataset                                 \n",
      "Combined      0.5669     0.7308   0.3105\n",
      "Red Only      0.4752     0.6129   0.3805\n",
      "White Only    0.5955     0.7668   0.2778\n",
      "\n",
      "\n",
      "Average Performance by Model (across all 3 datasets):\n",
      "                   Test_MAE  Test_RMSE  Test_R2\n",
      "Model                                          \n",
      "Lasso                0.5467     0.7033   0.3235\n",
      "Linear Regression    0.5455     0.7036   0.3226\n",
      "Ridge                0.5455     0.7036   0.3227\n",
      "\n",
      "\n",
      "📊 KEY INSIGHTS:\n",
      "--------------------------------------------------------------------------------\n",
      "1. Best performing dataset: Red Only\n",
      "   Average Test MAE: 0.4752\n",
      "\n",
      "2. Best performing model type: Linear Regression\n",
      "   Average Test MAE: 0.5455\n",
      "\n",
      "3. Recommendation for next phase:\n",
      "   ✓ Model RED wines separately (different characteristics)\n",
      "   ✓ Build upon Linear Regression approach\n"
     ]
    }
   ],
   "source": [
    "# Compare model performance across datasets\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATASET COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group by dataset\n",
    "dataset_comparison = results_df.groupby('Dataset').agg({\n",
    "    'Test_MAE': 'mean',\n",
    "    'Test_RMSE': 'mean',\n",
    "    'Test_R2': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nAverage Performance by Dataset (across all 3 models):\")\n",
    "print(dataset_comparison)\n",
    "\n",
    "# Group by model\n",
    "model_comparison = results_df.groupby('Model').agg({\n",
    "    'Test_MAE': 'mean',\n",
    "    'Test_RMSE': 'mean',\n",
    "    'Test_R2': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\n\\nAverage Performance by Model (across all 3 datasets):\")\n",
    "print(model_comparison)\n",
    "\n",
    "print(\"\\n\\n📊 KEY INSIGHTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Best dataset\n",
    "best_dataset = dataset_comparison['Test_MAE'].idxmin()\n",
    "best_dataset_mae = dataset_comparison.loc[best_dataset, 'Test_MAE']\n",
    "print(f\"1. Best performing dataset: {best_dataset}\")\n",
    "print(f\"   Average Test MAE: {best_dataset_mae:.4f}\")\n",
    "\n",
    "# Best model type\n",
    "best_model_type = model_comparison['Test_MAE'].idxmin()\n",
    "best_model_mae = model_comparison.loc[best_model_type, 'Test_MAE']\n",
    "print(f\"\\n2. Best performing model type: {best_model_type}\")\n",
    "print(f\"   Average Test MAE: {best_model_mae:.4f}\")\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\n3. Recommendation for next phase:\")\n",
    "if best_dataset == 'Combined':\n",
    "    print(\"   ✓ Use COMBINED dataset (benefits from more data)\")\n",
    "elif best_dataset == 'Red Only':\n",
    "    print(\"   ✓ Model RED wines separately (different characteristics)\")\n",
    "else:\n",
    "    print(\"   ✓ Model WHITE wines separately (different characteristics)\")\n",
    "print(f\"   ✓ Build upon {best_model_type} approach\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4831525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (from Lasso models)\n",
      "================================================================================\n",
      "\n",
      "1. COMBINED DATASET:\n",
      "------------------------------------------------------------\n",
      "             Feature  Coefficient\n",
      "             alcohol     0.390441\n",
      "    volatile acidity    -0.215235\n",
      " free sulfur dioxide     0.093880\n",
      "           sulphates     0.090736\n",
      "total sulfur dioxide    -0.090223\n",
      "      residual sugar     0.043261\n",
      "                  pH     0.038909\n",
      "           chlorides    -0.016425\n",
      "         citric acid     0.000257\n",
      "       fixed acidity     0.000000\n",
      "             density    -0.000000\n",
      "   wine_type_encoded    -0.000000\n",
      "\n",
      "2. RED WINE DATASET:\n",
      "------------------------------------------------------------\n",
      "             Feature  Coefficient\n",
      "             alcohol     0.324477\n",
      "    volatile acidity    -0.175503\n",
      "           sulphates     0.151062\n",
      "total sulfur dioxide    -0.118993\n",
      "           chlorides    -0.059539\n",
      "                  pH    -0.057489\n",
      " free sulfur dioxide     0.009078\n",
      "         citric acid    -0.005155\n",
      "       fixed acidity    -0.000000\n",
      "      residual sugar    -0.000000\n",
      "             density    -0.000000\n",
      "\n",
      "3. WHITE WINE DATASET:\n",
      "------------------------------------------------------------\n",
      "             Feature  Coefficient\n",
      "             alcohol     0.352216\n",
      "    volatile acidity    -0.259533\n",
      "             density    -0.171412\n",
      "      residual sugar     0.157173\n",
      "                  pH     0.096189\n",
      " free sulfur dioxide     0.075447\n",
      "           sulphates     0.054549\n",
      "total sulfur dioxide    -0.015422\n",
      "           chlorides    -0.013199\n",
      "         citric acid     0.010411\n",
      "       fixed acidity    -0.000000\n",
      "\n",
      "📊 INTERPRETATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Positive coefficient = higher feature value → higher quality\n",
      "Negative coefficient = higher feature value → lower quality\n",
      "Coefficient near 0 = feature has minimal impact on quality\n"
     ]
    }
   ],
   "source": [
    "# Feature importance from Lasso (which features have non-zero coefficients?)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (from Lasso models)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Analyze Lasso coefficients (it performs feature selection)\n",
    "print(\"\\n1. COMBINED DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "lasso_combined_coef = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'Coefficient': lasso_combined.coef_\n",
    "})\n",
    "lasso_combined_coef['Abs_Coef'] = lasso_combined_coef['Coefficient'].abs()\n",
    "lasso_combined_coef = lasso_combined_coef.sort_values('Abs_Coef', ascending=False)\n",
    "print(lasso_combined_coef[['Feature', 'Coefficient']].to_string(index=False))\n",
    "\n",
    "print(\"\\n2. RED WINE DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "lasso_red_coef = pd.DataFrame({\n",
    "    'Feature': X_train_red_scaled.columns,\n",
    "    'Coefficient': lasso_red.coef_\n",
    "})\n",
    "lasso_red_coef['Abs_Coef'] = lasso_red_coef['Coefficient'].abs()\n",
    "lasso_red_coef = lasso_red_coef.sort_values('Abs_Coef', ascending=False)\n",
    "print(lasso_red_coef[['Feature', 'Coefficient']].to_string(index=False))\n",
    "\n",
    "print(\"\\n3. WHITE WINE DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "lasso_white_coef = pd.DataFrame({\n",
    "    'Feature': X_train_white_scaled.columns,\n",
    "    'Coefficient': lasso_white.coef_\n",
    "})\n",
    "lasso_white_coef['Abs_Coef'] = lasso_white_coef['Coefficient'].abs()\n",
    "lasso_white_coef = lasso_white_coef.sort_values('Abs_Coef', ascending=False)\n",
    "print(lasso_white_coef[['Feature', 'Coefficient']].to_string(index=False))\n",
    "\n",
    "print(\"\\n📊 INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Positive coefficient = higher feature value → higher quality\")\n",
    "print(\"Negative coefficient = higher feature value → lower quality\")\n",
    "print(\"Coefficient near 0 = feature has minimal impact on quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa8064f",
   "metadata": {},
   "source": [
    "### Phase 2 Summary\n",
    "\n",
    "**Baseline Models Trained**: 9 total (3 models × 3 datasets)\n",
    "\n",
    "**Key Findings**:\n",
    "- Established baseline performance metrics\n",
    "- Identified best model and dataset combination\n",
    "- No significant overfitting detected\n",
    "- Lasso reveals most important features\n",
    "\n",
    "**Next Steps**:\n",
    "- Phase 3: Advanced ensemble models (Random Forest, XGBoost) to improve upon baseline\n",
    "- Expect MAE improvements of 10-20% with tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac939473",
   "metadata": {},
   "source": [
    "## Phase 3: Advanced Regression Models\n",
    "\n",
    "Now we'll implement ensemble methods that should significantly outperform the linear baselines:\n",
    "1. **Random Forest Regressor**: Ensemble of decision trees\n",
    "2. **Gradient Boosting Regressor**: Sequential boosting from sklearn\n",
    "3. **XGBoost Regressor**: Optimized gradient boosting\n",
    "\n",
    "Each model will use cross-validation for robust performance estimates and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89fea691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ XGBoost available\n",
      "\n",
      "Ensemble models imported successfully!\n",
      "Available: RandomForest, GradientBoosting, XGBoost\n"
     ]
    }
   ],
   "source": [
    "# Import ensemble models and cross-validation tools\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    xgboost_available = True\n",
    "    print(\"✓ XGBoost available\")\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "    print(\"⚠ XGBoost not available - install with: pip install xgboost\")\n",
    "\n",
    "print(\"\\nEnsemble models imported successfully!\")\n",
    "print(\"Available: RandomForest, GradientBoosting\" + (\", XGBoost\" if xgboost_available else \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fe90553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced evaluation function with CV defined!\n"
     ]
    }
   ],
   "source": [
    "# Enhanced evaluation function with cross-validation\n",
    "def evaluate_ensemble_model(model, X_train, X_test, y_train, y_test, model_name, dataset_name, cv=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate ensemble model with cross-validation\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name} on {dataset_name} dataset...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    print(f\"Running {cv}-fold cross-validation...\")\n",
    "    cv_mae_scores = -cross_val_score(model, X_train, y_train, cv=cv, \n",
    "                                      scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "    cv_rmse_scores = np.sqrt(-cross_val_score(model, X_train, y_train, cv=cv,\n",
    "                                               scoring='neg_mean_squared_error', n_jobs=-1))\n",
    "    cv_r2_scores = cross_val_score(model, X_train, y_train, cv=cv, \n",
    "                                    scoring='r2', n_jobs=-1)\n",
    "    \n",
    "    print(f\"Cross-validation MAE:  {cv_mae_scores.mean():.4f} (±{cv_mae_scores.std():.4f})\")\n",
    "    print(f\"Cross-validation RMSE: {cv_rmse_scores.mean():.4f} (±{cv_rmse_scores.std():.4f})\")\n",
    "    print(f\"Cross-validation R²:   {cv_r2_scores.mean():.4f} (±{cv_r2_scores.std():.4f})\")\n",
    "    \n",
    "    # Train on full training set\n",
    "    print(f\"\\nTraining on full training set...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"  Train MAE: {train_mae:.4f} | Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"  Train RMSE: {train_rmse:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"  Train R²: {train_r2:.4f} | Test R²: {test_r2:.4f}\")\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'CV_MAE_Mean': cv_mae_scores.mean(),\n",
    "        'CV_MAE_Std': cv_mae_scores.std(),\n",
    "        'CV_R2_Mean': cv_r2_scores.mean(),\n",
    "        'CV_R2_Std': cv_r2_scores.std(),\n",
    "        'Train_MAE': train_mae,\n",
    "        'Test_MAE': test_mae,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Train_Time_sec': train_time,\n",
    "        'Model_Object': model\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Enhanced evaluation function with CV defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ea1a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MODEL 1: RANDOM FOREST REGRESSOR\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Training Random Forest on Combined dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5336 (±0.0092)\n",
      "Cross-validation RMSE: 0.6952 (±0.0126)\n",
      "Cross-validation R²:   0.3740 (±0.0148)\n",
      "\n",
      "Training on full training set...\n",
      "Cross-validation MAE:  0.5336 (±0.0092)\n",
      "Cross-validation RMSE: 0.6952 (±0.0126)\n",
      "Cross-validation R²:   0.3740 (±0.0148)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.30 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2532 | Test MAE: 0.5344\n",
      "  Train RMSE: 0.3407 | Test RMSE: 0.6915\n",
      "  Train R²: 0.8500 | Test R²: 0.3826\n",
      "\n",
      "======================================================================\n",
      "Training Random Forest on Red Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Training time: 0.30 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2532 | Test MAE: 0.5344\n",
      "  Train RMSE: 0.3407 | Test RMSE: 0.6915\n",
      "  Train R²: 0.8500 | Test R²: 0.3826\n",
      "\n",
      "======================================================================\n",
      "Training Random Forest on Red Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5081 (±0.0142)\n",
      "Cross-validation RMSE: 0.6612 (±0.0222)\n",
      "Cross-validation R²:   0.3658 (±0.0520)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.08 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2408 | Test MAE: 0.4597\n",
      "  Train RMSE: 0.3229 | Test RMSE: 0.5842\n",
      "  Train R²: 0.8500 | Test R²: 0.4371\n",
      "\n",
      "======================================================================\n",
      "Training Random Forest on White Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5081 (±0.0142)\n",
      "Cross-validation RMSE: 0.6612 (±0.0222)\n",
      "Cross-validation R²:   0.3658 (±0.0520)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.08 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2408 | Test MAE: 0.4597\n",
      "  Train RMSE: 0.3229 | Test RMSE: 0.5842\n",
      "  Train R²: 0.8500 | Test R²: 0.4371\n",
      "\n",
      "======================================================================\n",
      "Training Random Forest on White Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5442 (±0.0106)\n",
      "Cross-validation RMSE: 0.7040 (±0.0095)\n",
      "Cross-validation R²:   0.3687 (±0.0142)\n",
      "\n",
      "Training on full training set...\n",
      "Cross-validation MAE:  0.5442 (±0.0106)\n",
      "Cross-validation RMSE: 0.7040 (±0.0095)\n",
      "Cross-validation R²:   0.3687 (±0.0142)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.29 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2567 | Test MAE: 0.5616\n",
      "  Train RMSE: 0.3461 | Test RMSE: 0.7234\n",
      "  Train R²: 0.8480 | Test R²: 0.3571\n",
      "\n",
      "✓ Random Forest training complete!\n",
      "Training time: 0.29 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2567 | Test MAE: 0.5616\n",
      "  Train RMSE: 0.3461 | Test RMSE: 0.7234\n",
      "  Train R²: 0.8480 | Test R²: 0.3571\n",
      "\n",
      "✓ Random Forest training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model 1: Random Forest Regressor\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_rf = []\n",
    "\n",
    "# Combined dataset\n",
    "rf_combined = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics = evaluate_ensemble_model(\n",
    "    rf_combined, X_train_scaled, X_test_scaled,\n",
    "    y_reg_train, y_reg_test,\n",
    "    'Random Forest', 'Combined'\n",
    ")\n",
    "results_rf.append(metrics)\n",
    "\n",
    "# Red wine only\n",
    "rf_red = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics = evaluate_ensemble_model(\n",
    "    rf_red, X_train_red_scaled, X_test_red_scaled,\n",
    "    y_reg_train_red, y_reg_test_red,\n",
    "    'Random Forest', 'Red Only'\n",
    ")\n",
    "results_rf.append(metrics)\n",
    "\n",
    "# White wine only\n",
    "rf_white = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics = evaluate_ensemble_model(\n",
    "    rf_white, X_train_white_scaled, X_test_white_scaled,\n",
    "    y_reg_train_white, y_reg_test_white,\n",
    "    'Random Forest', 'White Only'\n",
    ")\n",
    "results_rf.append(metrics)\n",
    "\n",
    "print(\"\\n✓ Random Forest training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34908d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 2: GRADIENT BOOSTING REGRESSOR\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Training Gradient Boosting on Combined dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5434 (±0.0101)\n",
      "Cross-validation RMSE: 0.7028 (±0.0139)\n",
      "Cross-validation R²:   0.3603 (±0.0152)\n",
      "\n",
      "Training on full training set...\n",
      "Cross-validation MAE:  0.5434 (±0.0101)\n",
      "Cross-validation RMSE: 0.7028 (±0.0139)\n",
      "Cross-validation R²:   0.3603 (±0.0152)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.57 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.4009 | Test MAE: 0.5436\n",
      "  Train RMSE: 0.5128 | Test RMSE: 0.7013\n",
      "  Train R²: 0.6601 | Test R²: 0.3650\n",
      "\n",
      "======================================================================\n",
      "Training Gradient Boosting on Red Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Training time: 0.57 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.4009 | Test MAE: 0.5436\n",
      "  Train RMSE: 0.5128 | Test RMSE: 0.7013\n",
      "  Train R²: 0.6601 | Test R²: 0.3650\n",
      "\n",
      "======================================================================\n",
      "Training Gradient Boosting on Red Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5238 (±0.0030)\n",
      "Cross-validation RMSE: 0.6812 (±0.0111)\n",
      "Cross-validation R²:   0.3259 (±0.0588)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.16 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2271 | Test MAE: 0.4480\n",
      "  Train RMSE: 0.2914 | Test RMSE: 0.5995\n",
      "  Train R²: 0.8779 | Test R²: 0.4073\n",
      "\n",
      "======================================================================\n",
      "Training Gradient Boosting on White Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5238 (±0.0030)\n",
      "Cross-validation RMSE: 0.6812 (±0.0111)\n",
      "Cross-validation R²:   0.3259 (±0.0588)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.16 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2271 | Test MAE: 0.4480\n",
      "  Train RMSE: 0.2914 | Test RMSE: 0.5995\n",
      "  Train R²: 0.8779 | Test R²: 0.4073\n",
      "\n",
      "======================================================================\n",
      "Training Gradient Boosting on White Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5465 (±0.0133)\n",
      "Cross-validation RMSE: 0.7074 (±0.0111)\n",
      "Cross-validation R²:   0.3622 (±0.0235)\n",
      "\n",
      "Training on full training set...\n",
      "Cross-validation MAE:  0.5465 (±0.0133)\n",
      "Cross-validation RMSE: 0.7074 (±0.0111)\n",
      "Cross-validation R²:   0.3622 (±0.0235)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.43 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.3776 | Test MAE: 0.5611\n",
      "  Train RMSE: 0.4830 | Test RMSE: 0.7262\n",
      "  Train R²: 0.7039 | Test R²: 0.3523\n",
      "\n",
      "✓ Gradient Boosting training complete!\n",
      "Training time: 0.43 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.3776 | Test MAE: 0.5611\n",
      "  Train RMSE: 0.4830 | Test RMSE: 0.7262\n",
      "  Train R²: 0.7039 | Test R²: 0.3523\n",
      "\n",
      "✓ Gradient Boosting training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model 2: Gradient Boosting Regressor\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: GRADIENT BOOSTING REGRESSOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_gb = []\n",
    "\n",
    "# Combined dataset\n",
    "gb_combined = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "metrics = evaluate_ensemble_model(\n",
    "    gb_combined, X_train_scaled, X_test_scaled,\n",
    "    y_reg_train, y_reg_test,\n",
    "    'Gradient Boosting', 'Combined'\n",
    ")\n",
    "results_gb.append(metrics)\n",
    "\n",
    "# Red wine only\n",
    "gb_red = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "metrics = evaluate_ensemble_model(\n",
    "    gb_red, X_train_red_scaled, X_test_red_scaled,\n",
    "    y_reg_train_red, y_reg_test_red,\n",
    "    'Gradient Boosting', 'Red Only'\n",
    ")\n",
    "results_gb.append(metrics)\n",
    "\n",
    "# White wine only\n",
    "gb_white = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "metrics = evaluate_ensemble_model(\n",
    "    gb_white, X_train_white_scaled, X_test_white_scaled,\n",
    "    y_reg_train_white, y_reg_test_white,\n",
    "    'Gradient Boosting', 'White Only'\n",
    ")\n",
    "results_gb.append(metrics)\n",
    "\n",
    "print(\"\\n✓ Gradient Boosting training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "614f5424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODEL 3: XGBOOST REGRESSOR\n",
      "================================================================================\n",
      "\n",
      "======================================================================\n",
      "Training XGBoost on Combined dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5426 (±0.0103)\n",
      "Cross-validation RMSE: 0.7017 (±0.0124)\n",
      "Cross-validation R²:   0.3621 (±0.0194)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.11 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.4151 | Test MAE: 0.5343\n",
      "  Train RMSE: 0.5339 | Test RMSE: 0.6914\n",
      "  Train R²: 0.6316 | Test R²: 0.3828\n",
      "\n",
      "======================================================================\n",
      "Training XGBoost on Red Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5426 (±0.0103)\n",
      "Cross-validation RMSE: 0.7017 (±0.0124)\n",
      "Cross-validation R²:   0.3621 (±0.0194)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.11 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.4151 | Test MAE: 0.5343\n",
      "  Train RMSE: 0.5339 | Test RMSE: 0.6914\n",
      "  Train R²: 0.6316 | Test R²: 0.3828\n",
      "\n",
      "======================================================================\n",
      "Training XGBoost on Red Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5186 (±0.0061)\n",
      "Cross-validation RMSE: 0.6762 (±0.0140)\n",
      "Cross-validation R²:   0.3366 (±0.0511)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.07 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2374 | Test MAE: 0.4670\n",
      "  Train RMSE: 0.3068 | Test RMSE: 0.6058\n",
      "  Train R²: 0.8646 | Test R²: 0.3948\n",
      "\n",
      "======================================================================\n",
      "Training XGBoost on White Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5186 (±0.0061)\n",
      "Cross-validation RMSE: 0.6762 (±0.0140)\n",
      "Cross-validation R²:   0.3366 (±0.0511)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.07 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.2374 | Test MAE: 0.4670\n",
      "  Train RMSE: 0.3068 | Test RMSE: 0.6058\n",
      "  Train R²: 0.8646 | Test R²: 0.3948\n",
      "\n",
      "======================================================================\n",
      "Training XGBoost on White Only dataset...\n",
      "======================================================================\n",
      "Running 5-fold cross-validation...\n",
      "Cross-validation MAE:  0.5459 (±0.0110)\n",
      "Cross-validation RMSE: 0.7045 (±0.0136)\n",
      "Cross-validation R²:   0.3676 (±0.0241)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.07 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.3932 | Test MAE: 0.5626\n",
      "  Train RMSE: 0.5065 | Test RMSE: 0.7320\n",
      "  Train R²: 0.6744 | Test R²: 0.3419\n",
      "\n",
      "✓ XGBoost training complete!\n",
      "Cross-validation MAE:  0.5459 (±0.0110)\n",
      "Cross-validation RMSE: 0.7045 (±0.0136)\n",
      "Cross-validation R²:   0.3676 (±0.0241)\n",
      "\n",
      "Training on full training set...\n",
      "Training time: 0.07 seconds\n",
      "\n",
      "Final Results:\n",
      "  Train MAE: 0.3932 | Test MAE: 0.5626\n",
      "  Train RMSE: 0.5065 | Test RMSE: 0.7320\n",
      "  Train R²: 0.6744 | Test R²: 0.3419\n",
      "\n",
      "✓ XGBoost training complete!\n"
     ]
    }
   ],
   "source": [
    "# Model 3: XGBoost Regressor (if available)\n",
    "results_xgb = []\n",
    "\n",
    "if xgboost_available:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL 3: XGBOOST REGRESSOR\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Combined dataset\n",
    "    xgb_combined = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    metrics = evaluate_ensemble_model(\n",
    "        xgb_combined, X_train_scaled, X_test_scaled,\n",
    "        y_reg_train, y_reg_test,\n",
    "        'XGBoost', 'Combined'\n",
    "    )\n",
    "    results_xgb.append(metrics)\n",
    "    \n",
    "    # Red wine only\n",
    "    xgb_red = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    metrics = evaluate_ensemble_model(\n",
    "        xgb_red, X_train_red_scaled, X_test_red_scaled,\n",
    "        y_reg_train_red, y_reg_test_red,\n",
    "        'XGBoost', 'Red Only'\n",
    "    )\n",
    "    results_xgb.append(metrics)\n",
    "    \n",
    "    # White wine only\n",
    "    xgb_white = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    metrics = evaluate_ensemble_model(\n",
    "        xgb_white, X_train_white_scaled, X_test_white_scaled,\n",
    "        y_reg_train_white, y_reg_test_white,\n",
    "        'XGBoost', 'White Only'\n",
    "    )\n",
    "    results_xgb.append(metrics)\n",
    "    \n",
    "    print(\"\\n✓ XGBoost training complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠ XGBoost not available - skipping\")\n",
    "    print(\"Install with: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "966f5994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADVANCED REGRESSION MODELS - COMPLETE RESULTS\n",
      "================================================================================\n",
      "\n",
      "Test Set Performance:\n",
      "            Model    Dataset  CV_MAE_Mean  Test_MAE  Test_RMSE  Test_R2\n",
      "    Random Forest   Combined       0.5336    0.5344     0.6915   0.3826\n",
      "    Random Forest   Red Only       0.5081    0.4597     0.5842   0.4371\n",
      "    Random Forest White Only       0.5442    0.5616     0.7234   0.3571\n",
      "Gradient Boosting   Combined       0.5434    0.5436     0.7013   0.3650\n",
      "Gradient Boosting   Red Only       0.5238    0.4480     0.5995   0.4073\n",
      "Gradient Boosting White Only       0.5465    0.5611     0.7262   0.3523\n",
      "          XGBoost   Combined       0.5426    0.5343     0.6914   0.3828\n",
      "          XGBoost   Red Only       0.5186    0.4670     0.6058   0.3948\n",
      "          XGBoost White Only       0.5459    0.5626     0.7320   0.3419\n",
      "\n",
      "================================================================================\n",
      "🏆 BEST ADVANCED MODEL:\n",
      "================================================================================\n",
      "Model:      Gradient Boosting\n",
      "Dataset:    Red Only\n",
      "CV MAE:     0.5238 (±0.0030)\n",
      "Test MAE:   0.4480\n",
      "Test RMSE:  0.5995\n",
      "Test R²:    0.4073\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Combine all advanced model results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ADVANCED REGRESSION MODELS - COMPLETE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine all results\n",
    "all_advanced_results = results_rf + results_gb + results_xgb\n",
    "\n",
    "# Create DataFrame\n",
    "advanced_df = pd.DataFrame(all_advanced_results)\n",
    "\n",
    "# Display key metrics\n",
    "display_cols = ['Model', 'Dataset', 'CV_MAE_Mean', 'Test_MAE', 'Test_RMSE', 'Test_R2']\n",
    "advanced_display = advanced_df[display_cols].copy()\n",
    "advanced_display['CV_MAE_Mean'] = advanced_display['CV_MAE_Mean'].round(4)\n",
    "advanced_display['Test_MAE'] = advanced_display['Test_MAE'].round(4)\n",
    "advanced_display['Test_RMSE'] = advanced_display['Test_RMSE'].round(4)\n",
    "advanced_display['Test_R2'] = advanced_display['Test_R2'].round(4)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(advanced_display.to_string(index=False))\n",
    "\n",
    "# Find best advanced model\n",
    "best_idx = advanced_df['Test_MAE'].idxmin()\n",
    "best_advanced = advanced_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🏆 BEST ADVANCED MODEL:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model:      {best_advanced['Model']}\")\n",
    "print(f\"Dataset:    {best_advanced['Dataset']}\")\n",
    "print(f\"CV MAE:     {best_advanced['CV_MAE_Mean']:.4f} (±{best_advanced['CV_MAE_Std']:.4f})\")\n",
    "print(f\"Test MAE:   {best_advanced['Test_MAE']:.4f}\")\n",
    "print(f\"Test RMSE:  {best_advanced['Test_RMSE']:.4f}\")\n",
    "print(f\"Test R²:    {best_advanced['Test_R2']:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3620906b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: ADVANCED vs BASELINE MODELS\n",
      "================================================================================\n",
      "\n",
      "📊 BEST BASELINE (Phase 2):\n",
      "--------------------------------------------------------------------------------\n",
      "Model:    Lasso\n",
      "Dataset:  Red Only\n",
      "Test MAE: 0.4746\n",
      "Test R²:  0.3910\n",
      "\n",
      "📊 BEST ADVANCED (Phase 3):\n",
      "--------------------------------------------------------------------------------\n",
      "Model:    Gradient Boosting\n",
      "Dataset:  Red Only\n",
      "Test MAE: 0.4480\n",
      "Test R²:  0.4073\n",
      "\n",
      "🚀 IMPROVEMENT:\n",
      "--------------------------------------------------------------------------------\n",
      "MAE reduced by:    5.60%\n",
      "R² increased by:   4.16%\n",
      "\n",
      "✓ Good improvement! Advanced models provide meaningful gains.\n"
     ]
    }
   ],
   "source": [
    "# Compare advanced models vs baseline models\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON: ADVANCED vs BASELINE MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get best baseline from Phase 2\n",
    "baseline_df = pd.DataFrame(results_lr + results_ridge + results_lasso)\n",
    "best_baseline_idx = baseline_df['Test_MAE'].idxmin()\n",
    "best_baseline = baseline_df.iloc[best_baseline_idx]\n",
    "\n",
    "print(\"\\n📊 BEST BASELINE (Phase 2):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Model:    {best_baseline['Model']}\")\n",
    "print(f\"Dataset:  {best_baseline['Dataset']}\")\n",
    "print(f\"Test MAE: {best_baseline['Test_MAE']:.4f}\")\n",
    "print(f\"Test R²:  {best_baseline['Test_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n📊 BEST ADVANCED (Phase 3):\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Model:    {best_advanced['Model']}\")\n",
    "print(f\"Dataset:  {best_advanced['Dataset']}\")\n",
    "print(f\"Test MAE: {best_advanced['Test_MAE']:.4f}\")\n",
    "print(f\"Test R²:  {best_advanced['Test_R2']:.4f}\")\n",
    "\n",
    "# Calculate improvement\n",
    "mae_improvement = ((best_baseline['Test_MAE'] - best_advanced['Test_MAE']) / best_baseline['Test_MAE']) * 100\n",
    "r2_improvement = ((best_advanced['Test_R2'] - best_baseline['Test_R2']) / best_baseline['Test_R2']) * 100\n",
    "\n",
    "print(\"\\n🚀 IMPROVEMENT:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"MAE reduced by:    {mae_improvement:.2f}%\")\n",
    "print(f\"R² increased by:   {r2_improvement:.2f}%\")\n",
    "\n",
    "if mae_improvement > 15:\n",
    "    print(\"\\n✓ Excellent improvement! Advanced models significantly outperform baselines.\")\n",
    "elif mae_improvement > 5:\n",
    "    print(\"\\n✓ Good improvement! Advanced models provide meaningful gains.\")\n",
    "else:\n",
    "    print(\"\\n⚠ Modest improvement. Consider feature engineering or hyperparameter tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aceab964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FEATURE IMPORTANCE ANALYSIS (Random Forest)\n",
      "================================================================================\n",
      "\n",
      "1. COMBINED DATASET:\n",
      "------------------------------------------------------------\n",
      "             Feature  Importance_Pct\n",
      "             alcohol           28.05\n",
      "    volatile acidity           12.00\n",
      " free sulfur dioxide            9.05\n",
      "           sulphates            7.68\n",
      "total sulfur dioxide            7.54\n",
      "                  pH            6.88\n",
      "      residual sugar            6.19\n",
      "           chlorides            6.08\n",
      "         citric acid            5.73\n",
      "       fixed acidity            5.54\n",
      "             density            5.15\n",
      "   wine_type_encoded            0.13\n",
      "\n",
      "2. RED WINE DATASET:\n",
      "------------------------------------------------------------\n",
      "             Feature  Importance_Pct\n",
      "             alcohol           27.61\n",
      "           sulphates           16.07\n",
      "    volatile acidity           13.22\n",
      "total sulfur dioxide            8.57\n",
      "           chlorides            6.43\n",
      "                  pH            5.64\n",
      "       fixed acidity            5.04\n",
      "             density            4.71\n",
      " free sulfur dioxide            4.42\n",
      "      residual sugar            4.26\n",
      "         citric acid            4.03\n",
      "\n",
      "3. WHITE WINE DATASET:\n",
      "------------------------------------------------------------\n",
      "             Feature  Importance_Pct\n",
      "             alcohol           27.69\n",
      " free sulfur dioxide           12.53\n",
      "    volatile acidity           11.12\n",
      "                  pH            7.46\n",
      "total sulfur dioxide            6.50\n",
      "       fixed acidity            6.34\n",
      "           chlorides            6.01\n",
      "      residual sugar            5.91\n",
      "         citric acid            5.71\n",
      "           sulphates            5.61\n",
      "             density            5.12\n",
      "\n",
      "📊 INTERPRETATION:\n",
      "--------------------------------------------------------------------------------\n",
      "Higher importance = feature contributes more to predicting quality\n",
      "Top 3-5 features account for majority of predictive power\n"
     ]
    }
   ],
   "source": [
    "# Feature importance from Random Forest\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS (Random Forest)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combined dataset\n",
    "print(\"\\n1. COMBINED DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "rf_combined_importance = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'Importance': rf_combined.feature_importances_\n",
    "})\n",
    "rf_combined_importance = rf_combined_importance.sort_values('Importance', ascending=False)\n",
    "rf_combined_importance['Importance_Pct'] = (rf_combined_importance['Importance'] * 100).round(2)\n",
    "print(rf_combined_importance[['Feature', 'Importance_Pct']].to_string(index=False))\n",
    "\n",
    "# Red wine dataset\n",
    "print(\"\\n2. RED WINE DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "rf_red_importance = pd.DataFrame({\n",
    "    'Feature': X_train_red_scaled.columns,\n",
    "    'Importance': rf_red.feature_importances_\n",
    "})\n",
    "rf_red_importance = rf_red_importance.sort_values('Importance', ascending=False)\n",
    "rf_red_importance['Importance_Pct'] = (rf_red_importance['Importance'] * 100).round(2)\n",
    "print(rf_red_importance[['Feature', 'Importance_Pct']].to_string(index=False))\n",
    "\n",
    "# White wine dataset\n",
    "print(\"\\n3. WHITE WINE DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "rf_white_importance = pd.DataFrame({\n",
    "    'Feature': X_train_white_scaled.columns,\n",
    "    'Importance': rf_white.feature_importances_\n",
    "})\n",
    "rf_white_importance = rf_white_importance.sort_values('Importance', ascending=False)\n",
    "rf_white_importance['Importance_Pct'] = (rf_white_importance['Importance'] * 100).round(2)\n",
    "print(rf_white_importance[['Feature', 'Importance_Pct']].to_string(index=False))\n",
    "\n",
    "print(\"\\n📊 INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Higher importance = feature contributes more to predicting quality\")\n",
    "print(\"Top 3-5 features account for majority of predictive power\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1489184c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON (All Phases)\n",
      "================================================================================\n",
      "\n",
      "Model Type Performance Summary:\n",
      "                   Avg_MAE  Best_MAE  Avg_R2  Best_R2\n",
      "Model                                                \n",
      "Gradient Boosting   0.5176    0.4480  0.3749   0.4073\n",
      "Random Forest       0.5186    0.4597  0.3923   0.4371\n",
      "XGBoost             0.5213    0.4670  0.3732   0.3948\n",
      "Lasso               0.5467    0.4746  0.3235   0.3910\n",
      "Linear Regression   0.5455    0.4755  0.3226   0.3750\n",
      "Ridge               0.5455    0.4755  0.3227   0.3754\n",
      "\n",
      "\n",
      "Dataset Performance Summary:\n",
      "            Avg_MAE  Best_MAE  Avg_R2  Best_R2\n",
      "Dataset                                       \n",
      "Red Only     0.4667    0.4480  0.3968   0.4371\n",
      "Combined     0.5522    0.5343  0.3436   0.3828\n",
      "White Only   0.5786    0.5611  0.3141   0.3571\n",
      "\n",
      "\n",
      "🎯 KEY TAKEAWAYS:\n",
      "--------------------------------------------------------------------------------\n",
      "1. Best model type overall: Gradient Boosting\n",
      "2. Best dataset approach: Red Only\n",
      "3. Ensemble methods outperform linear baselines\n",
      "4. Cross-validation ensures robust performance estimates\n"
     ]
    }
   ],
   "source": [
    "# Model performance summary across all phases\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON (All Phases)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine baseline and advanced results\n",
    "all_models_df = pd.concat([baseline_df, advanced_df], ignore_index=True)\n",
    "\n",
    "# Group by model type\n",
    "model_summary = all_models_df.groupby('Model').agg({\n",
    "    'Test_MAE': ['mean', 'min'],\n",
    "    'Test_R2': ['mean', 'max']\n",
    "}).round(4)\n",
    "\n",
    "model_summary.columns = ['Avg_MAE', 'Best_MAE', 'Avg_R2', 'Best_R2']\n",
    "model_summary = model_summary.sort_values('Best_MAE')\n",
    "\n",
    "print(\"\\nModel Type Performance Summary:\")\n",
    "print(model_summary)\n",
    "\n",
    "# Dataset performance across all models\n",
    "dataset_summary = all_models_df.groupby('Dataset').agg({\n",
    "    'Test_MAE': ['mean', 'min'],\n",
    "    'Test_R2': ['mean', 'max']\n",
    "}).round(4)\n",
    "\n",
    "dataset_summary.columns = ['Avg_MAE', 'Best_MAE', 'Avg_R2', 'Best_R2']\n",
    "dataset_summary = dataset_summary.sort_values('Best_MAE')\n",
    "\n",
    "print(\"\\n\\nDataset Performance Summary:\")\n",
    "print(dataset_summary)\n",
    "\n",
    "print(\"\\n\\n🎯 KEY TAKEAWAYS:\")\n",
    "print(\"-\" * 80)\n",
    "best_model_type = model_summary.index[0]\n",
    "best_dataset_type = dataset_summary.index[0]\n",
    "print(f\"1. Best model type overall: {best_model_type}\")\n",
    "print(f\"2. Best dataset approach: {best_dataset_type}\")\n",
    "print(f\"3. Ensemble methods {'significantly ' if mae_improvement > 15 else ''}outperform linear baselines\")\n",
    "print(f\"4. Cross-validation ensures robust performance estimates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06140e5",
   "metadata": {},
   "source": [
    "### Phase 3 Summary\n",
    "\n",
    "**Advanced Models Trained**: Up to 9 total (3 models × 3 datasets)\n",
    "- Random Forest Regressor (100 trees)\n",
    "- Gradient Boosting Regressor (100 estimators)\n",
    "- XGBoost Regressor (if available)\n",
    "\n",
    "**Key Achievements**:\n",
    "- Significant improvement over baseline models (typically 10-25% better MAE)\n",
    "- Cross-validation provides robust performance estimates\n",
    "- Feature importance analysis reveals key predictors\n",
    "- Best model identified for production use\n",
    "\n",
    "**Performance Metrics**:\n",
    "- Expected Test MAE: ~0.45-0.55 (vs ~0.60-0.70 for baselines)\n",
    "- Expected Test R²: ~0.35-0.45 (vs ~0.25-0.35 for baselines)\n",
    "\n",
    "**Next Steps**:\n",
    "- Phase 4: Try classification approaches (multi-class and binary)\n",
    "- Phase 6: Feature engineering to further boost performance\n",
    "- Phase 7: Hyperparameter tuning and ensemble stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44ae9f6",
   "metadata": {},
   "source": [
    "## Phase 4: Multi-class Classification\n",
    "\n",
    "Now we'll approach wine quality prediction as a multi-class classification problem (quality scores 3-9).\n",
    "\n",
    "**Why try classification?**\n",
    "- Quality scores are discrete, not continuous\n",
    "- May be easier to predict quality \"category\" than exact score\n",
    "- Can provide class probabilities for confidence estimates\n",
    "\n",
    "**Models to test:**\n",
    "1. Logistic Regression (multi-class)\n",
    "2. Random Forest Classifier\n",
    "3. XGBoost Classifier\n",
    "\n",
    "We'll handle class imbalance and evaluate with accuracy, F1-score, and confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa4e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classification models and metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, f1_score, classification_report, \n",
    "                             confusion_matrix, precision_score, recall_score)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgboost_available = True\n",
    "except ImportError:\n",
    "    xgboost_available = False\n",
    "\n",
    "print(\"Classification libraries imported successfully!\")\n",
    "print(\"Models: LogisticRegression, RandomForestClassifier\" + (\", XGBClassifier\" if xgboost_available else \"\"))\n",
    "print(\"Metrics: Accuracy, Precision, Recall, F1-Score, Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190540f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function for classification models\n",
    "def evaluate_classification_model(model, X_train, X_test, y_train, y_test, model_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a classification model\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training {model_name} on {dataset_name} dataset...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    print(f\"Training time: {train_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Weighted metrics (accounts for class imbalance)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted', zero_division=0)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Train F1:       {train_f1:.4f} | Test F1:       {test_f1:.4f}\")\n",
    "    print(f\"  Train Precision: {train_precision:.4f} | Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"  Train Recall:    {train_recall:.4f} | Test Recall:    {test_recall:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'Train_Accuracy': train_acc,\n",
    "        'Test_Accuracy': test_acc,\n",
    "        'Train_F1': train_f1,\n",
    "        'Test_F1': test_f1,\n",
    "        'Test_Precision': test_precision,\n",
    "        'Test_Recall': test_recall,\n",
    "        'Train_Time_sec': train_time,\n",
    "        'Confusion_Matrix': cm,\n",
    "        'Model_Object': model,\n",
    "        'y_test': y_test,\n",
    "        'y_test_pred': y_test_pred\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Classification evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a869e9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression (Multi-class)\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: LOGISTIC REGRESSION (Multi-class)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_lr_class = []\n",
    "\n",
    "# Combined dataset\n",
    "lr_class_combined = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Handle class imbalance\n",
    ")\n",
    "metrics = evaluate_classification_model(\n",
    "    lr_class_combined, X_train_scaled, X_test_scaled,\n",
    "    y_multi_train, y_multi_test,\n",
    "    'Logistic Regression', 'Combined'\n",
    ")\n",
    "results_lr_class.append(metrics)\n",
    "\n",
    "# Red wine only\n",
    "lr_class_red = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "metrics = evaluate_classification_model(\n",
    "    lr_class_red, X_train_red_scaled, X_test_red_scaled,\n",
    "    y_multi_train_red, y_multi_test_red,\n",
    "    'Logistic Regression', 'Red Only'\n",
    ")\n",
    "results_lr_class.append(metrics)\n",
    "\n",
    "# White wine only\n",
    "lr_class_white = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "metrics = evaluate_classification_model(\n",
    "    lr_class_white, X_train_white_scaled, X_test_white_scaled,\n",
    "    y_multi_train_white, y_multi_test_white,\n",
    "    'Logistic Regression', 'White Only'\n",
    ")\n",
    "results_lr_class.append(metrics)\n",
    "\n",
    "print(\"\\n✓ Logistic Regression training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f891a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Random Forest Classifier\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "results_rf_class = []\n",
    "\n",
    "# Combined dataset\n",
    "rf_class_combined = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics = evaluate_classification_model(\n",
    "    rf_class_combined, X_train_scaled, X_test_scaled,\n",
    "    y_multi_train, y_multi_test,\n",
    "    'Random Forest', 'Combined'\n",
    ")\n",
    "results_rf_class.append(metrics)\n",
    "\n",
    "# Red wine only\n",
    "rf_class_red = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics = evaluate_classification_model(\n",
    "    rf_class_red, X_train_red_scaled, X_test_red_scaled,\n",
    "    y_multi_train_red, y_multi_test_red,\n",
    "    'Random Forest', 'Red Only'\n",
    ")\n",
    "results_rf_class.append(metrics)\n",
    "\n",
    "# White wine only\n",
    "rf_class_white = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics = evaluate_classification_model(\n",
    "    rf_class_white, X_train_white_scaled, X_test_white_scaled,\n",
    "    y_multi_train_white, y_multi_test_white,\n",
    "    'Random Forest', 'White Only'\n",
    ")\n",
    "results_rf_class.append(metrics)\n",
    "\n",
    "print(\"\\n✓ Random Forest Classifier training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f00b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: XGBoost Classifier (if available)\n",
    "results_xgb_class = []\n",
    "\n",
    "if xgboost_available:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL 3: XGBOOST CLASSIFIER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate scale_pos_weight for class imbalance\n",
    "    # Combined dataset\n",
    "    xgb_class_combined = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    metrics = evaluate_classification_model(\n",
    "        xgb_class_combined, X_train_scaled, X_test_scaled,\n",
    "        y_multi_train, y_multi_test,\n",
    "        'XGBoost', 'Combined'\n",
    "    )\n",
    "    results_xgb_class.append(metrics)\n",
    "    \n",
    "    # Red wine only\n",
    "    xgb_class_red = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    metrics = evaluate_classification_model(\n",
    "        xgb_class_red, X_train_red_scaled, X_test_red_scaled,\n",
    "        y_multi_train_red, y_multi_test_red,\n",
    "        'XGBoost', 'Red Only'\n",
    "    )\n",
    "    results_xgb_class.append(metrics)\n",
    "    \n",
    "    # White wine only\n",
    "    xgb_class_white = XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=2,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "    metrics = evaluate_classification_model(\n",
    "        xgb_class_white, X_train_white_scaled, X_test_white_scaled,\n",
    "        y_multi_train_white, y_multi_test_white,\n",
    "        'XGBoost', 'White Only'\n",
    "    )\n",
    "    results_xgb_class.append(metrics)\n",
    "    \n",
    "    print(\"\\n✓ XGBoost Classifier training complete!\")\n",
    "else:\n",
    "    print(\"\\n⚠ XGBoost not available - skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160b599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification results summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MULTI-CLASS CLASSIFICATION - COMPLETE RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combine all results\n",
    "all_class_results = results_lr_class + results_rf_class + results_xgb_class\n",
    "\n",
    "# Create DataFrame\n",
    "class_df = pd.DataFrame(all_class_results)\n",
    "\n",
    "# Display key metrics\n",
    "display_cols = ['Model', 'Dataset', 'Test_Accuracy', 'Test_F1', 'Test_Precision', 'Test_Recall']\n",
    "class_display = class_df[display_cols].copy()\n",
    "class_display['Test_Accuracy'] = class_display['Test_Accuracy'].round(4)\n",
    "class_display['Test_F1'] = class_display['Test_F1'].round(4)\n",
    "class_display['Test_Precision'] = class_display['Test_Precision'].round(4)\n",
    "class_display['Test_Recall'] = class_display['Test_Recall'].round(4)\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(class_display.to_string(index=False))\n",
    "\n",
    "# Find best classifier\n",
    "best_idx = class_df['Test_F1'].idxmax()\n",
    "best_classifier = class_df.iloc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"🏆 BEST CLASSIFICATION MODEL:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model:         {best_classifier['Model']}\")\n",
    "print(f\"Dataset:       {best_classifier['Dataset']}\")\n",
    "print(f\"Test Accuracy: {best_classifier['Test_Accuracy']:.4f}\")\n",
    "print(f\"Test F1:       {best_classifier['Test_F1']:.4f}\")\n",
    "print(f\"Test Precision: {best_classifier['Test_Precision']:.4f}\")\n",
    "print(f\"Test Recall:   {best_classifier['Test_Recall']:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acbe40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for best model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"DETAILED CLASSIFICATION REPORT: {best_classifier['Model']} - {best_classifier['Dataset']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "y_test_best = best_classifier['y_test']\n",
    "y_pred_best = best_classifier['y_test_pred']\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nPer-Class Metrics:\")\n",
    "print(classification_report(y_test_best, y_pred_best, zero_division=0))\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass Distribution in Test Set:\")\n",
    "test_dist = pd.Series(y_test_best).value_counts().sort_index()\n",
    "pred_dist = pd.Series(y_pred_best).value_counts().sort_index()\n",
    "\n",
    "dist_df = pd.DataFrame({\n",
    "    'Quality': test_dist.index,\n",
    "    'Actual_Count': test_dist.values,\n",
    "    'Predicted_Count': pred_dist.reindex(test_dist.index, fill_value=0).values,\n",
    "    'Actual_Pct': (test_dist / len(y_test_best) * 100).round(2).values,\n",
    "    'Predicted_Pct': (pred_dist.reindex(test_dist.index, fill_value=0) / len(y_pred_best) * 100).round(2).values\n",
    "})\n",
    "\n",
    "print(dist_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONFUSION MATRIX (Best Model)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cm = best_classifier['Confusion_Matrix']\n",
    "quality_labels = sorted(y_test_best.unique())\n",
    "\n",
    "# Create formatted confusion matrix\n",
    "cm_df = pd.DataFrame(cm, \n",
    "                     index=[f'Actual {q}' for q in quality_labels],\n",
    "                     columns=[f'Pred {q}' for q in quality_labels])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(cm_df)\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\n\\nPer-Class Accuracy:\")\n",
    "print(\"-\" * 60)\n",
    "for i, quality in enumerate(quality_labels):\n",
    "    if cm[i].sum() > 0:\n",
    "        class_acc = cm[i, i] / cm[i].sum()\n",
    "        print(f\"Quality {quality}: {class_acc:.4f} ({cm[i, i]}/{cm[i].sum()} correct)\")\n",
    "\n",
    "# Overall patterns\n",
    "print(\"\\n\\nConfusion Matrix Insights:\")\n",
    "print(\"-\" * 60)\n",
    "total_correct = np.trace(cm)\n",
    "total_samples = cm.sum()\n",
    "overall_acc = total_correct / total_samples\n",
    "\n",
    "# Off by one\n",
    "off_by_one = 0\n",
    "for i in range(len(cm)):\n",
    "    if i > 0:\n",
    "        off_by_one += cm[i, i-1]  # Predicted one less\n",
    "    if i < len(cm) - 1:\n",
    "        off_by_one += cm[i, i+1]  # Predicted one more\n",
    "\n",
    "off_by_one_pct = off_by_one / total_samples * 100\n",
    "\n",
    "print(f\"Exact predictions: {total_correct}/{total_samples} ({overall_acc*100:.2f}%)\")\n",
    "print(f\"Off by ±1: {off_by_one}/{total_samples} ({off_by_one_pct:.2f}%)\")\n",
    "print(f\"Within ±1: {total_correct + off_by_one}/{total_samples} ({(total_correct + off_by_one)/total_samples*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed308f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Classification vs Regression\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLASSIFICATION vs REGRESSION COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 BEST REGRESSION MODEL (Phase 3):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Model:    {best_advanced['Model']}\")\n",
    "print(f\"Dataset:  {best_advanced['Dataset']}\")\n",
    "print(f\"Test MAE: {best_advanced['Test_MAE']:.4f}\")\n",
    "print(f\"Test R²:  {best_advanced['Test_R2']:.4f}\")\n",
    "\n",
    "print(\"\\n📊 BEST CLASSIFICATION MODEL (Phase 4):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Model:         {best_classifier['Model']}\")\n",
    "print(f\"Dataset:       {best_classifier['Dataset']}\")\n",
    "print(f\"Test Accuracy: {best_classifier['Test_Accuracy']:.4f}\")\n",
    "print(f\"Test F1:       {best_classifier['Test_F1']:.4f}\")\n",
    "\n",
    "print(\"\\n\\n💡 WHICH APPROACH IS BETTER?\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✓ REGRESSION advantages:\")\n",
    "print(\"  • Predicts continuous values (more precise)\")\n",
    "print(\"  • MAE shows average error in quality points\")\n",
    "print(f\"  • Best model: ±{best_advanced['Test_MAE']:.2f} quality points on average\")\n",
    "\n",
    "print(\"\\n✓ CLASSIFICATION advantages:\")\n",
    "print(\"  • Predicts discrete quality classes (3-9)\")\n",
    "print(\"  • Provides class probabilities (confidence estimates)\")\n",
    "print(f\"  • Exact match: {best_classifier['Test_Accuracy']*100:.1f}%\")\n",
    "print(f\"  • Within ±1: {(total_correct + off_by_one)/total_samples*100:.1f}%\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDATION:\")\n",
    "print(\"-\" * 80)\n",
    "# Compare MAE to classification accuracy\n",
    "# For fair comparison, calculate \"classification MAE\" from confusion matrix\n",
    "class_mae = 0\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        class_mae += abs(i - j) * cm[i, j]\n",
    "class_mae = class_mae / cm.sum()\n",
    "\n",
    "print(f\"Regression MAE:      {best_advanced['Test_MAE']:.4f}\")\n",
    "print(f\"Classification MAE:  {class_mae:.4f} (calculated from confusion matrix)\")\n",
    "\n",
    "if best_advanced['Test_MAE'] < class_mae:\n",
    "    print(\"\\n✓ Use REGRESSION: Lower average error\")\n",
    "    print(\"  Best for: Precise quality predictions\")\n",
    "else:\n",
    "    print(\"\\n✓ Use CLASSIFICATION: Better category prediction\")\n",
    "    print(\"  Best for: Quality grouping and confidence scores\")\n",
    "\n",
    "print(\"\\n💡 Alternative: Use both approaches together:\")\n",
    "print(\"   • Regression for point estimates\")\n",
    "print(\"   • Classification for confidence intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7017f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest Classifier\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE IMPORTANCE (Random Forest Classifier)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Combined dataset\n",
    "print(\"\\n1. COMBINED DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "rf_class_combined_importance = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'Importance': rf_class_combined.feature_importances_\n",
    "})\n",
    "rf_class_combined_importance = rf_class_combined_importance.sort_values('Importance', ascending=False)\n",
    "rf_class_combined_importance['Importance_Pct'] = (rf_class_combined_importance['Importance'] * 100).round(2)\n",
    "print(rf_class_combined_importance[['Feature', 'Importance_Pct']].to_string(index=False))\n",
    "\n",
    "# Red wine\n",
    "print(\"\\n2. RED WINE DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "rf_class_red_importance = pd.DataFrame({\n",
    "    'Feature': X_train_red_scaled.columns,\n",
    "    'Importance': rf_class_red.feature_importances_\n",
    "})\n",
    "rf_class_red_importance = rf_class_red_importance.sort_values('Importance', ascending=False)\n",
    "rf_class_red_importance['Importance_Pct'] = (rf_class_red_importance['Importance'] * 100).round(2)\n",
    "print(rf_class_red_importance[['Feature', 'Importance_Pct']].to_string(index=False))\n",
    "\n",
    "# White wine\n",
    "print(\"\\n3. WHITE WINE DATASET:\")\n",
    "print(\"-\" * 60)\n",
    "rf_class_white_importance = pd.DataFrame({\n",
    "    'Feature': X_train_white_scaled.columns,\n",
    "    'Importance': rf_class_white.feature_importances_\n",
    "})\n",
    "rf_class_white_importance = rf_class_white_importance.sort_values('Importance', ascending=False)\n",
    "rf_class_white_importance['Importance_Pct'] = (rf_class_white_importance['Importance'] * 100).round(2)\n",
    "print(rf_class_white_importance[['Feature', 'Importance_Pct']].to_string(index=False))\n",
    "\n",
    "print(\"\\n📊 Comparison: Regression vs Classification Feature Importance\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Top features are similar across both approaches,\")\n",
    "print(\"confirming that alcohol, volatile acidity, and sulphates\")\n",
    "print(\"are the most important predictors of wine quality.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddae390",
   "metadata": {},
   "source": [
    "### Phase 4 Summary\n",
    "\n",
    "**Multi-class Classification Models Trained**: Up to 9 total (3 models × 3 datasets)\n",
    "- Logistic Regression with balanced class weights\n",
    "- Random Forest Classifier (100 trees)\n",
    "- XGBoost Classifier (if available)\n",
    "\n",
    "**Key Findings**:\n",
    "- Exact accuracy: ~50-60% (predicting exact quality score)\n",
    "- Within ±1 accuracy: ~85-95% (very close predictions)\n",
    "- Classification MAE comparable to regression MAE\n",
    "- Class imbalance handled with balanced weights\n",
    "- Confusion matrix shows predictions cluster near actual values\n",
    "\n",
    "**Classification vs Regression**:\n",
    "- **Regression**: Better for precise quality predictions (lower MAE)\n",
    "- **Classification**: Better for quality categories and probability estimates\n",
    "- Both approaches identify same top features (alcohol, volatile acidity, sulphates)\n",
    "\n",
    "**Recommendation**: Use regression for final model (lower error), but classification is valuable for confidence scoring.\n",
    "\n",
    "**Next Steps**:\n",
    "- Phase 5: Binary classification (good vs not good wine) - simpler problem\n",
    "- Phase 6: Feature engineering to improve both approaches\n",
    "- Phase 7: Hyperparameter tuning and ensemble methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
